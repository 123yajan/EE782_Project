{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9874016d81bd494786f09af1876b25ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b9bc4166eafd48258b7a24f1364490c4",
              "IPY_MODEL_3b6f35fe2191443c9008a80ef2893de0",
              "IPY_MODEL_68bd4037a4b64e5dbb9b5735605c8142"
            ],
            "layout": "IPY_MODEL_ee0b99e7f7ae4380a8c75171611f42ff"
          }
        },
        "b9bc4166eafd48258b7a24f1364490c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f54d28d88b9d49cb81443b59fb83ed6f",
            "placeholder": "​",
            "style": "IPY_MODEL_b4e67f915dcc4b79a4de0cbb752c33e9",
            "value": "Building Memory: 100%"
          }
        },
        "3b6f35fe2191443c9008a80ef2893de0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_396951c9d0794d8a9d953082f7ec055e",
            "max": 32,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8b5fd9c065e84d46927708cc2176ee9e",
            "value": 32
          }
        },
        "68bd4037a4b64e5dbb9b5735605c8142": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_319718c9c91e4def903bcbb61109ff51",
            "placeholder": "​",
            "style": "IPY_MODEL_83a5c75314ab44e3b3b58ff8a63ab8c4",
            "value": " 32/32 [00:00&lt;00:00, 97.71it/s]"
          }
        },
        "ee0b99e7f7ae4380a8c75171611f42ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f54d28d88b9d49cb81443b59fb83ed6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4e67f915dcc4b79a4de0cbb752c33e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "396951c9d0794d8a9d953082f7ec055e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b5fd9c065e84d46927708cc2176ee9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "319718c9c91e4def903bcbb61109ff51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83a5c75314ab44e3b3b58ff8a63ab8c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5944ff39937c48d0979f91ce8d62ab22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_12f1d6fa7cd54731a53077b782fe8492",
              "IPY_MODEL_cfd4049d29bd4323902e7a348e3d351a",
              "IPY_MODEL_56a062c53ef444d7bc84ee02ab25c0e3"
            ],
            "layout": "IPY_MODEL_2c4cd550418745a4ab22e7cfa14a12b4"
          }
        },
        "12f1d6fa7cd54731a53077b782fe8492": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ab8f41bf38c474bb3514e0d082eec9b",
            "placeholder": "​",
            "style": "IPY_MODEL_cb09e80fbb254030af578ef03f2a3461",
            "value": "Evaluating on Test Set: 100%"
          }
        },
        "cfd4049d29bd4323902e7a348e3d351a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e21bb38764c444db87aeb8b31d8f744d",
            "max": 32,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ce557f3828e84a56881ecf322cb58571",
            "value": 32
          }
        },
        "56a062c53ef444d7bc84ee02ab25c0e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9c33546a173472c8d9d081d65b4f6da",
            "placeholder": "​",
            "style": "IPY_MODEL_68959dff99d84856bd15862e4229cbb9",
            "value": " 32/32 [00:01&lt;00:00, 28.53it/s]"
          }
        },
        "2c4cd550418745a4ab22e7cfa14a12b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ab8f41bf38c474bb3514e0d082eec9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb09e80fbb254030af578ef03f2a3461": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e21bb38764c444db87aeb8b31d8f744d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce557f3828e84a56881ecf322cb58571": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a9c33546a173472c8d9d081d65b4f6da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68959dff99d84856bd15862e4229cbb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xn-UliTjEI3H",
        "outputId": "7038a9bd-d20c-406f-c92c-803e091470df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'PatchTST'...\n",
            "remote: Enumerating objects: 352, done.\u001b[K\n",
            "remote: Counting objects: 100% (129/129), done.\u001b[K\n",
            "remote: Compressing objects: 100% (51/51), done.\u001b[K\n",
            "remote: Total 352 (delta 83), reused 78 (delta 78), pack-reused 223 (from 1)\u001b[K\n",
            "Receiving objects: 100% (352/352), 12.92 MiB | 40.47 MiB/s, done.\n",
            "Resolving deltas: 100% (155/155), done.\n",
            "Cloned PatchTST repository.\n",
            "Cloning into 'HopCPT'...\n",
            "remote: Enumerating objects: 2116, done.\u001b[K\n",
            "remote: Counting objects: 100% (352/352), done.\u001b[K\n",
            "remote: Compressing objects: 100% (221/221), done.\u001b[K\n",
            "remote: Total 2116 (delta 156), reused 282 (delta 125), pack-reused 1764 (from 1)\u001b[K\n",
            "Receiving objects: 100% (2116/2116), 270.93 MiB | 26.73 MiB/s, done.\n",
            "Resolving deltas: 100% (357/357), done.\n",
            "Updating files: 100% (3161/3161), done.\n",
            "Cloned HopCPT repository.\n",
            "✅ Libraries installed and repositories cloned!\n"
          ]
        }
      ],
      "source": [
        "# Install standard libraries\n",
        "!pip install torch pandas numpy matplotlib scikit-learn -q\n",
        "import os\n",
        "\n",
        "# Clone PatchTST repository\n",
        "if not os.path.exists('PatchTST'):\n",
        "    !git clone https://github.com/yuqinie98/PatchTST.git\n",
        "    print(\"Cloned PatchTST repository.\")\n",
        "else:\n",
        "    print(\"PatchTST repository already exists.\")\n",
        "\n",
        "# Clone HopCPT repository\n",
        "if not os.path.exists('HopCPT'):\n",
        "    !git clone https://github.com/ml-jku/HopCPT.git\n",
        "    print(\"Cloned HopCPT repository.\")\n",
        "else:\n",
        "    print(\"HopCPT repository already exists.\")\n",
        "\n",
        "print(\"✅ Libraries installed and repositories cloned!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "\n",
        "PROJECT_DIR = \"/content/drive/MyDrive/PatchCPT_Project\"\n",
        "if not os.path.exists(PROJECT_DIR):\n",
        "    os.makedirs(PROJECT_DIR)\n",
        "    print(f\"Created project directory at: {PROJECT_DIR}\")\n",
        "else:\n",
        "    print(f\"Project directory already exists at: {PROJECT_DIR}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrwynutyO9Dz",
        "outputId": "75b57845-4d2f-411d-ecac-e1e814021589"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Project directory already exists at: /content/drive/MyDrive/PatchCPT_Project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# --- Standard Libraries ---\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import sys\n",
        "from tqdm.auto import tqdm\n",
        "from typing import Optional\n",
        "\n",
        "# --- PyTorch Libraries ---\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# --- Scikit-Learn ---\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# We add the PARENT 'supervised' folder to the path.\n",
        "# This allows 'models/PatchTST.py' to successfully import 'layers/RevIN.py'.\n",
        "sys.path.append('/content/PatchTST/PatchTST_supervised')\n",
        "\n",
        "try:\n",
        "    # Now that the parent is on the path, we can import from 'models'\n",
        "    # and the model itself can successfully import from its sibling 'layers' folder.\n",
        "    from models.PatchTST import Model as PatchTST\n",
        "    print(\"✅ Successfully imported PatchTST (class 'Model').\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error importing PatchTST: {e}\")\n",
        "    print(\"   Please ensure the path '/content/PatchTST/PatchTST_supervised' exists.\")\n",
        "\n",
        "# This code is correct and defines the Hopfield class manually.\n",
        "class Hopfield(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_size: int,\n",
        "                 hidden_size: Optional[int] = None,\n",
        "                 output_size: Optional[int] = None,\n",
        "                 num_heads: int = 8,\n",
        "                 dropout: float = 0.0,\n",
        "                 scaling: Optional[float] = None,\n",
        "                 **kwargs):\n",
        "        super(Hopfield, self).__init__()\n",
        "        if hidden_size is None: hidden_size = input_size\n",
        "        if output_size is None: output_size = input_size\n",
        "        self.input_size, self.hidden_size, self.output_size = input_size, hidden_size, output_size\n",
        "        self.num_heads = num_heads\n",
        "        assert self.hidden_size % self.num_heads == 0, 'hidden_size must be divisible by num_heads.'\n",
        "        self.head_size = self.hidden_size // self.num_heads\n",
        "        scaling = scaling if scaling is not None else self.head_size ** (1/4)\n",
        "        self.scaling = torch.nn.Parameter(torch.tensor([scaling]), requires_grad=False)\n",
        "        self.W_Q = torch.nn.Linear(in_features=self.input_size, out_features=self.hidden_size, bias=False)\n",
        "        self.W_K = torch.nn.Linear(in_features=self.input_size, out_features=self.hidden_size, bias=False)\n",
        "        self.W_V = torch.nn.Linear(in_features=self.input_size, out_features=self.hidden_size, bias=False)\n",
        "        self.W_O = torch.nn.Linear(in_features=self.hidden_size, out_features=self.output_size, bias=False)\n",
        "        self.dropout = torch.nn.Dropout(p=dropout)\n",
        "        self.softmax = torch.nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self,\n",
        "                query: torch.Tensor,\n",
        "                key: torch.Tensor,\n",
        "                value: torch.Tensor,\n",
        "                return_raw_associations: bool = False) -> torch.Tensor:\n",
        "        batch_size, query_len, _ = query.shape\n",
        "        key_len, value_len = key.shape[1], value.shape[1]\n",
        "        assert key_len == value_len, 'key and value must have same sequence length.'\n",
        "        Q = self.W_Q(query).view(batch_size, query_len, self.num_heads, self.head_size).transpose(1, 2)\n",
        "        K = self.W_K(key).view(batch_size, key_len, self.num_heads, self.head_size).transpose(1, 2)\n",
        "        V = self.W_V(value).view(batch_size, value_len, self.num_heads, self.head_size).transpose(1, 2)\n",
        "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scaling\n",
        "        attn_weights = self.softmax(attn_scores)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "        output = torch.matmul(attn_weights, V)\n",
        "        output = output.transpose(1, 2).contiguous().view(batch_size, query_len, self.hidden_size)\n",
        "        output = self.W_O(output)\n",
        "        return (output, attn_scores) if return_raw_associations else (output, attn_weights)\n",
        "\n",
        "print(\"✅ Hopfield (MHN) class manually defined.\")\n",
        "\n",
        "# --- 4. Device and Seed Setup ---\n",
        "# We also run the device and seed setup here.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "def set_seed(seed_value=42):\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed_value)\n",
        "        torch.cuda.manual_seed_all(seed_value)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)\n",
        "print(\"✅ Device set and seed fixed.\")\n",
        "print(\"\\n--- All Setup Complete ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWB7Z6FCSZNY",
        "outputId": "a5ae0371-1dc2-4275-e6ff-d1724d3441b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Successfully imported PatchTST (class 'Model').\n",
            "✅ Hopfield (MHN) class manually defined.\n",
            "Using device: cuda\n",
            "✅ Device set and seed fixed.\n",
            "\n",
            "--- All Setup Complete ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# --- This is the RevIN class that PatchTST was failing to import ---\n",
        "class RevIN(nn.Module):\n",
        "    def __init__(self, num_features: int, eps=1e-5, affine=True, subtract_last=False):\n",
        "        \"\"\"\n",
        "        :param num_features: the number of features or channels\n",
        "        :param eps: a value added for numerical stability\n",
        "        :param affine: if True, RevIN has learnable parameters\n",
        "        \"\"\"\n",
        "        super(RevIN, self).__init__()\n",
        "        self.num_features = num_features\n",
        "        self.eps = eps\n",
        "        self.affine = affine\n",
        "        self.subtract_last = subtract_last\n",
        "        if self.affine:\n",
        "            self._init_params()\n",
        "\n",
        "    def forward(self, x, mode:str):\n",
        "        if mode == 'norm':\n",
        "            self._get_statistics(x)\n",
        "            x = self._normalize(x)\n",
        "        elif mode == 'denorm':\n",
        "            x = self._denormalize(x)\n",
        "        else: raise NotImplementedError\n",
        "        return x\n",
        "\n",
        "    def _init_params(self):\n",
        "        # initialize RevIN params: (C,)\n",
        "        self.affine_weight = nn.Parameter(torch.ones(self.num_features))\n",
        "        self.affine_bias = nn.Parameter(torch.zeros(self.num_features))\n",
        "\n",
        "    def _get_statistics(self, x):\n",
        "        dim2reduce = tuple(range(1, x.ndim-1))\n",
        "        if self.subtract_last:\n",
        "            self.last = x[:,-1,:].unsqueeze(1)\n",
        "        else:\n",
        "            self.mean = torch.mean(x, dim=dim2reduce, keepdim=True).detach()\n",
        "        self.stdev = torch.sqrt(torch.var(x, dim=dim2reduce, keepdim=True, unbiased=False) + self.eps).detach()\n",
        "\n",
        "    def _normalize(self, x):\n",
        "        if self.subtract_last:\n",
        "            x = x - self.last\n",
        "        else:\n",
        "            x = x - self.mean\n",
        "        x = x / self.stdev\n",
        "        if self.affine:\n",
        "            x = x * self.affine_weight\n",
        "            x = x + self.affine_bias\n",
        "        return x\n",
        "\n",
        "    def _denormalize(self, x):\n",
        "        if self.affine:\n",
        "            x = x - self.affine_bias\n",
        "            x = x / (self.affine_weight + self.eps*self.eps)\n",
        "        x = x * self.stdev\n",
        "        if self.subtract_last:\n",
        "            x = x + self.last\n",
        "        else:\n",
        "            x = x + self.mean\n",
        "        return x\n",
        "\n",
        "print(\"✅ RevIN class defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UNFxS31fFUL",
        "outputId": "992999e3-f747-4e63-b8ae-ad1cd12150dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ RevIN class defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from typing import Optional\n",
        "\n",
        "# --- Helper Layers (from layers/PatchTST_layers.py) ---\n",
        "\n",
        "class TSTiEncoder(nn.Module):\n",
        "    def __init__(self, c_in, patch_num, patch_len, n_layers=3, d_model=128, n_heads=16, d_k=None, d_v=None,\n",
        "                 d_ff=256, norm='BatchNorm', attn_dropout=0., dropout=0., act=\"gelu\", store_attn=False,\n",
        "                 key_padding_mask='patch', padding_var=None, attn_mask=None, res_attention=True, pre_norm=False,\n",
        "                 pe='zeros', learn_pe=True, verbose=False, **kwargs):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.patch_num = patch_num\n",
        "        self.patch_len = patch_len\n",
        "\n",
        "        # Patching\n",
        "        self.patching = nn.Linear(patch_len, d_model)\n",
        "\n",
        "        # Positional embedding\n",
        "        if pe == 'zeros':\n",
        "            self.W_pos = nn.Parameter(torch.zeros(1, patch_num, d_model))\n",
        "        elif pe == 'sincos':\n",
        "            self.W_pos = nn.Parameter(torch.zeros(1, patch_num, d_model))\n",
        "            self.W_pos = self.positional_encoding(patch_num, d_model)\n",
        "        else:\n",
        "            raise ValueError(f\"pe: {pe} not implemented\")\n",
        "\n",
        "        self.learn_pe = learn_pe\n",
        "        if learn_pe:\n",
        "            self.W_pos = nn.Parameter(self.W_pos)\n",
        "\n",
        "        # Residual dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Encoder\n",
        "        self.layers = nn.ModuleList([\n",
        "            nn.TransformerEncoderLayer(\n",
        "                d_model=d_model,\n",
        "                nhead=n_heads,\n",
        "                dim_feedforward=d_ff,\n",
        "                dropout=dropout,\n",
        "                activation=act,\n",
        "                batch_first=True\n",
        "            ) for _ in range(n_layers)\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(d_model) if pre_norm else nn.Identity()\n",
        "\n",
        "    def forward(self, x) -> torch.Tensor:                                              # x: [B, C, N, P]\n",
        "\n",
        "        n_vars = x.shape[1]\n",
        "\n",
        "        # Input encoding\n",
        "        x = x.permute(0, 1, 3, 2)                                                      # x: [B, C, P, N]\n",
        "        x = self.patching(x)                                                         # x: [B, C, N, D]\n",
        "\n",
        "        x = x.reshape(x.shape[0] * x.shape[1], x.shape[2], x.shape[3])                 # x: [B*C, N, D]\n",
        "        x = self.dropout(x + self.W_pos)                                             # x: [B*C, N, D]\n",
        "\n",
        "        # Encoder\n",
        "        x = self.norm(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        x = x.reshape(-1, n_vars, self.patch_num, x.shape[-1])                        # x: [B, C, N, D]\n",
        "        x = x.permute(0, 1, 3, 2)                                                      # x: [B, C, D, N]\n",
        "\n",
        "        return x, None  # Return None for attns\n",
        "\n",
        "\n",
        "# --- Main Model Class (from models/PatchTST.py) ---\n",
        "\n",
        "class PatchTST(nn.Module):\n",
        "    def __init__(self, configs, **kwargs):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # load parameters\n",
        "        c_in = configs.enc_in\n",
        "        context_window = configs.seq_len\n",
        "        target_window = configs.pred_len\n",
        "\n",
        "        n_layers = configs.e_layers\n",
        "        n_heads = configs.n_heads\n",
        "        d_model = configs.d_model\n",
        "        d_ff = configs.d_ff\n",
        "        dropout = configs.dropout\n",
        "        fc_dropout = configs.fc_dropout\n",
        "        head_dropout = configs.head_dropout\n",
        "\n",
        "        individual = configs.individual\n",
        "\n",
        "        patch_len = configs.patch_len\n",
        "        stride = configs.stride\n",
        "        padding_patch = configs.padding_patch\n",
        "\n",
        "        revin = configs.revin\n",
        "        affine = configs.affine\n",
        "        subtract_last = configs.subtract_last\n",
        "\n",
        "        decomposition = configs.decomposition\n",
        "        kernel_size = configs.kernel_size\n",
        "\n",
        "\n",
        "        # RevIN\n",
        "        self.revin = revin\n",
        "        if self.revin:\n",
        "            self.revin_layer = RevIN(c_in, affine=affine, subtract_last=subtract_last)\n",
        "        else:\n",
        "            self.revin_layer = nn.Identity()\n",
        "\n",
        "        # Patching\n",
        "        self.patch_len = patch_len\n",
        "        self.stride = stride\n",
        "        self.padding_patch = padding_patch\n",
        "        self.patch_num = int((context_window - patch_len)/stride + 1)\n",
        "        if padding_patch == 'end': # can be modified to handle more general cases\n",
        "            self.padding_patch_layer = nn.ReplicationPad1d((0, stride))\n",
        "            self.patch_num += 1\n",
        "\n",
        "        # Backbone\n",
        "        self.backbone = TSTiEncoder(c_in, patch_num=self.patch_num, patch_len=patch_len, n_layers=n_layers,\n",
        "                                  d_model=d_model, n_heads=n_heads, d_ff=d_ff, attn_dropout=0,\n",
        "                                  dropout=dropout, act='gelu', res_attention=True, **kwargs)\n",
        "\n",
        "        # Head\n",
        "        self.head_nf = d_model * self.patch_num\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Flatten(start_dim=-2),\n",
        "            nn.Linear(self.head_nf, d_model),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(head_dropout),\n",
        "            nn.Linear(d_model, target_window)\n",
        "        )\n",
        "        self.individual = individual\n",
        "        if self.individual:\n",
        "            self.head = nn.ModuleList()\n",
        "            for i in range(c_in):\n",
        "                self.head.append(\n",
        "                    nn.Sequential(\n",
        "                        nn.Flatten(start_dim=-2),\n",
        "                        nn.Linear(self.head_nf, d_model),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(head_dropout),\n",
        "                        nn.Linear(d_model, target_window)\n",
        "                    )\n",
        "                )\n",
        "\n",
        "    def forward(self, x):                                              # x: [B, L, C]\n",
        "\n",
        "        # RevIN\n",
        "        x = self.revin_layer(x, 'norm')\n",
        "        x = x.permute(0, 2, 1)                                         # x: [B, C, L]\n",
        "\n",
        "        # Patching\n",
        "        if self.padding_patch == 'end':\n",
        "            x = self.padding_patch_layer(x)\n",
        "        x = x.unfold(dimension=2, size=self.patch_len, step=self.stride) # x: [B, C, N, P]\n",
        "        x = x.permute(0, 1, 3, 2)                                      # x: [B, C, P, N]\n",
        "\n",
        "        # Backbone\n",
        "        x, attns = self.backbone(x)                                    # x: [B, C, D, N]\n",
        "        x = x.permute(0, 1, 3, 2)                                      # x: [B, C, N, D]\n",
        "\n",
        "        # Head\n",
        "        if self.individual:\n",
        "            x_out = torch.zeros(x.shape[0], x.shape[1], self.head[0][3].out_features, device=x.device) # B, C, H\n",
        "            for i in range(x.shape[1]):\n",
        "                x_out[:, i, :] = self.head[i](x[:, i, :, :])\n",
        "            x = x_out\n",
        "            x = x.permute(0, 2, 1)                                   # x: [B, H, C]\n",
        "        else:\n",
        "            x = self.head(x)                                         # x: [B, C, H]\n",
        "            x = x.permute(0, 2, 1)                                   # x: [B, H, C]\n",
        "\n",
        "        # Denorm\n",
        "        x = self.revin_layer(x, 'denorm')\n",
        "        return x\n",
        "\n",
        "print(\"✅ PatchTST (local class) defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBAbZbHEfI1z",
        "outputId": "0267d673-959f-46fc-c32d-7236f6a7de42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ PatchTST (local class) defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Section 2"
      ],
      "metadata": {
        "id": "fN5r1HCyZnIm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Use the PROJECT_DIR we defined in Block 1.2\n",
        "PROJECT_DIR = \"/content/drive/MyDrive/PatchCPT_Project\"\n",
        "DATA_PATH = os.path.join(PROJECT_DIR, \"ETTh1.csv\")\n",
        "\n",
        "# Download the data if it doesn't exist\n",
        "if not os.path.exists(DATA_PATH):\n",
        "    print(\"Downloading ETTh1.csv data...\")\n",
        "    # This URL points to the raw CSV file in the dataset's GitHub repo\n",
        "    !wget -P {PROJECT_DIR} https://raw.githubusercontent.com/zhouhaoyi/ETDataset/main/ETT-small/ETTh1.csv\n",
        "    print(f\"Data saved to {DATA_PATH}\")\n",
        "else:\n",
        "    print(f\"Data already exists at {DATA_PATH}\")\n",
        "\n",
        "# Load the dataframe to check\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "print(\"\\nData loaded. Head:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6zDvTr3UtpZ",
        "outputId": "ad1411f5-69e0-482b-c383-d8978f5cbdc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data already exists at /content/drive/MyDrive/PatchCPT_Project/ETTh1.csv\n",
            "\n",
            "Data loaded. Head:\n",
            "                  date   HUFL   HULL   MUFL   MULL   LUFL   LULL         OT\n",
            "0  2016-07-01 00:00:00  5.827  2.009  1.599  0.462  4.203  1.340  30.531000\n",
            "1  2016-07-01 01:00:00  5.693  2.076  1.492  0.426  4.142  1.371  27.787001\n",
            "2  2016-07-01 02:00:00  5.157  1.741  1.279  0.355  3.777  1.218  27.787001\n",
            "3  2016-07-01 03:00:00  5.090  1.942  1.279  0.391  3.807  1.279  25.044001\n",
            "4  2016-07-01 04:00:00  5.358  1.942  1.492  0.462  3.868  1.279  21.948000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Global Model & Data Configuration ---\n",
        "\n",
        "class Config:\n",
        "    def __init__(self):\n",
        "        # --- Data Parameters ---\n",
        "        self.dataset = 'ETTh1'\n",
        "        self.data_path = DATA_PATH\n",
        "        self.target_col = 'OT'  # 'OT' (Oil Temperature) is the target for ETTh1\n",
        "        self.n_features = 7     # ETTh1 has 7 features (OT + 6 covariates)\n",
        "\n",
        "        # --- Windowing Parameters ---\n",
        "        # Look-back window (how much history to use)\n",
        "        self.seq_len = 96  # 96 hours (4 days)\n",
        "        # Prediction window (how far to forecast)\n",
        "        self.pred_len = 96 # 96 hours (4 days)\n",
        "\n",
        "        # --- PatchTST Model Parameters ---\n",
        "        self.patch_len = 16   # Length of each patch\n",
        "        self.stride = 8       # Stride between patches\n",
        "        self.d_model = 128    # Model dimension\n",
        "        self.n_heads = 8      # Number of attention heads\n",
        "        self.n_layers = 3     # Number of encoder layers\n",
        "        self.dropout = 0.1\n",
        "\n",
        "        # --- Training Parameters ---\n",
        "        self.batch_size = 32\n",
        "        self.n_epochs_patchtst = 20  # Total epochs for PatchTST\n",
        "        self.n_epochs_hopcpt = 20    # Total epochs for HopCPT module\n",
        "        self.learning_rate = 0.0001\n",
        "\n",
        "        # --- Conformal Prediction Parameters ---\n",
        "        self.alpha = 0.05  # For 95% prediction intervals (1 - alpha)\n",
        "\n",
        "        # --- \"Smoke Test\" Setting ---\n",
        "        # Set this to True to run on 1000 samples for debugging\n",
        "        # Set to False to run on the full dataset\n",
        "        self.smoke_test = True\n",
        "\n",
        "# Create an instance of the config\n",
        "config = Config()\n",
        "\n",
        "# Print the config to verify\n",
        "import json\n",
        "print(json.dumps(config.__dict__, indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1N7jsd0WYQDa",
        "outputId": "0834631e-2ba9-453c-e4e4-0ead1fb43f9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"dataset\": \"ETTh1\",\n",
            "  \"data_path\": \"/content/drive/MyDrive/PatchCPT_Project/ETTh1.csv\",\n",
            "  \"target_col\": \"OT\",\n",
            "  \"n_features\": 7,\n",
            "  \"seq_len\": 96,\n",
            "  \"pred_len\": 96,\n",
            "  \"patch_len\": 16,\n",
            "  \"stride\": 8,\n",
            "  \"d_model\": 128,\n",
            "  \"n_heads\": 8,\n",
            "  \"n_layers\": 3,\n",
            "  \"dropout\": 0.1,\n",
            "  \"batch_size\": 32,\n",
            "  \"n_epochs_patchtst\": 20,\n",
            "  \"n_epochs_hopcpt\": 20,\n",
            "  \"learning_rate\": 0.0001,\n",
            "  \"alpha\": 0.05,\n",
            "  \"smoke_test\": true\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "class TimeSeriesWindowDataset(Dataset):\n",
        "    def __init__(self, data, seq_len, pred_len, scale=True, scaler=None):\n",
        "        self.data = data\n",
        "        self.seq_len = seq_len\n",
        "        self.pred_len = pred_len\n",
        "        self.total_len = seq_len + pred_len\n",
        "        self.scale = scale\n",
        "\n",
        "        if self.scale:\n",
        "            if scaler is None:\n",
        "                # Fit a new scaler\n",
        "                self.scaler = StandardScaler()\n",
        "                self.data_scaled = self.scaler.fit_transform(self.data)\n",
        "            else:\n",
        "                # Use the provided (already fit) scaler\n",
        "                self.scaler = scaler\n",
        "                self.data_scaled = self.scaler.transform(self.data)\n",
        "        else:\n",
        "            self.data_scaled = self.data.values\n",
        "\n",
        "    def __len__(self):\n",
        "        # This determines the total number of samples we can create\n",
        "        return len(self.data_scaled) - self.total_len + 1\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # The input window\n",
        "        x = self.data_scaled[index : index + self.seq_len]\n",
        "\n",
        "        # The output (target) window\n",
        "        y = self.data_scaled[index + self.seq_len : index + self.total_len]\n",
        "\n",
        "        # Return as tensors\n",
        "        # We only care about the target column 'OT' for the y\n",
        "        # But the model in the repo is multivariate, so it predicts all 7 features\n",
        "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "    def get_scaler(self):\n",
        "        return self.scaler\n",
        "\n",
        "print(\"✅ TimeSeriesWindowDataset class defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iN37XyCUZsFU",
        "outputId": "c1e946d4-f0f2-4d08-9438-cfa1ec536497"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ TimeSeriesWindowDataset class defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# --- Load and Split Data ---\n",
        "df = pd.read_csv(config.data_path, parse_dates=['date'])\n",
        "df = df.drop('date', axis=1) # Drop the date column, keep only numbers\n",
        "\n",
        "# The standard ETT split (as used in the paper)\n",
        "# 60% Train, 20% Validation, 20% Test\n",
        "total_len = len(df)\n",
        "train_end = int(total_len * 0.6)\n",
        "val_end = int(total_len * 0.8)\n",
        "\n",
        "df_train = df.iloc[:train_end]\n",
        "df_val = df.iloc[train_end:val_end]\n",
        "df_test = df.iloc[val_end:]\n",
        "\n",
        "# --- Apply \"Smoke Test\" if enabled ---\n",
        "if config.smoke_test:\n",
        "    print(\"🔥🔥🔥 SMOKE TEST ENABLED 🔥🔥🔥\")\n",
        "    # We need enough data to create at least one window\n",
        "    smoke_len = 1000 + config.seq_len + config.pred_len\n",
        "    print(f\"Using a tiny subset of {smoke_len} rows for debugging.\") # <-- FIXED\n",
        "\n",
        "    df_train = df_train.iloc[:smoke_len]\n",
        "    df_val = df_val.iloc[:smoke_len]\n",
        "    df_test = df_test.iloc[:smoke_len]\n",
        "\n",
        "print(f\"Data shapes (Smoke Test={config.smoke_test}):\")\n",
        "print(f\"Train: {df_train.shape}\")\n",
        "print(f\"Val:   {df_val.shape}\")\n",
        "print(f\"Test:  {df_test.shape}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# --- Create Datasets ---\n",
        "\n",
        "# 1. Create and fit scaler on the TRAINING data\n",
        "train_dataset = TimeSeriesWindowDataset(\n",
        "    df_train, config.seq_len, config.pred_len, scale=True, scaler=None\n",
        ")\n",
        "train_scaler = train_dataset.get_scaler()\n",
        "\n",
        "# 2. Use the FITTED scaler to transform val and test data\n",
        "val_dataset = TimeSeriesWindowDataset(\n",
        "    df_val, config.seq_len, config.pred_len, scale=True, scaler=train_scaler\n",
        ")\n",
        "test_dataset = TimeSeriesWindowDataset(\n",
        "    df_test, config.seq_len, config.pred_len, scale=True, scaler=train_scaler\n",
        ")\n",
        "\n",
        "# --- Create DataLoaders ---\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config.batch_size,\n",
        "    shuffle=True,  # Shuffle training data\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=config.batch_size,\n",
        "    shuffle=False, # No shuffle for validation\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=config.batch_size,\n",
        "    shuffle=False, # No shuffle for test\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "print(\"\\nDataLoaders created.\")\n",
        "print(f\"Number of training batches: {len(train_loader)}\")\n",
        "print(f\"Number of validation batches: {len(val_loader)}\")\n",
        "print(f\"Number of test batches: {len(test_loader)}\")\n",
        "\n",
        "# --- Test one batch ---\n",
        "try:\n",
        "    x, y = next(iter(train_loader))\n",
        "    print(\"\\nSuccessfully loaded one batch:\")\n",
        "    print(f\"Input batch shape (x):  {x.shape}\")\n",
        "    print(f\"Output batch shape (y): {y.shape}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n❌ Error loading batch: {e}\")\n",
        "    print(\"This might happen if the 'smoke_test' dataset is too small for one batch.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZdK_kyAZul9",
        "outputId": "ff12197c-8a27-42ed-b55e-3c366a7c3ebb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔥🔥🔥 SMOKE TEST ENABLED 🔥🔥🔥\n",
            "Using a tiny subset of 1192 rows for debugging.\n",
            "Data shapes (Smoke Test=True):\n",
            "Train: (1192, 7)\n",
            "Val:   (1192, 7)\n",
            "Test:  (1192, 7)\n",
            "------------------------------\n",
            "\n",
            "DataLoaders created.\n",
            "Number of training batches: 32\n",
            "Number of validation batches: 32\n",
            "Number of test batches: 32\n",
            "\n",
            "Successfully loaded one batch:\n",
            "Input batch shape (x):  torch.Size([32, 96, 7])\n",
            "Output batch shape (y): torch.Size([32, 96, 7])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Section 3"
      ],
      "metadata": {
        "id": "WNLQdopTaHeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import time\n",
        "\n",
        "# --- 1. Instantiate Model, Optimizer, and Loss ---\n",
        "\n",
        "# Get the device (defined in Block 1.4)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Model configuration from our 'config' object\n",
        "patchtst_config = {\n",
        "    # --- Data parameters ---\n",
        "    'enc_in': config.n_features,\n",
        "    'c_out': config.n_features,\n",
        "    'seq_len': config.seq_len,\n",
        "    'pred_len': config.pred_len,\n",
        "\n",
        "    # --- Model structure parameters (FIXED) ---\n",
        "    'e_layers': config.n_layers,\n",
        "    'n_heads': config.n_heads,\n",
        "    'd_model': config.d_model,\n",
        "    'd_ff': config.d_model * 4,\n",
        "    'dropout': config.dropout,\n",
        "    'fc_dropout': config.dropout,\n",
        "    'head_dropout': config.dropout,\n",
        "    'individual': False,\n",
        "    'activation': 'gelu',\n",
        "    'patch_len': config.patch_len,\n",
        "    'stride': config.stride,\n",
        "\n",
        "    # --- Other required parameters from the repo (FIXED) ---\n",
        "    'padding_patch': 'end',\n",
        "    'revin': True,\n",
        "    'affine': True,\n",
        "    'subtract_last': False,\n",
        "    'decomposition': False,\n",
        "    'kernel_size': 25,          # <-- ADDED (a default for moving avg, even if not used)\n",
        "    'task_name': 'long_term_forecast',\n",
        "    'context_window': config.seq_len,\n",
        "    'target_window': config.pred_len,\n",
        "    'label_len': 0,\n",
        "    'd_k': None,\n",
        "    'd_v': None,\n",
        "    'norm': 'BatchNorm',\n",
        "    'attn_dropout': 0.0,\n",
        "    'act': 'gelu',\n",
        "    'key_padding_mask': 'patch',\n",
        "    'padding_var': None,\n",
        "    'attn_mask': None,\n",
        "    'res_attention': True,\n",
        "    'pre_norm': False,\n",
        "    'store_attn': False,\n",
        "    'pe': 'zeros',\n",
        "    'learn_pe': True,\n",
        "    'pretrain_head': False,\n",
        "    'head_type': 'flatten',\n",
        "    'verbose': False\n",
        "}\n",
        "\n",
        "\n",
        "# The 'Model' class we imported takes a custom 'config' object as input.\n",
        "class PatchTSTConfigObject:\n",
        "    def __init__(self, **kwargs):\n",
        "        self.__dict__.update(kwargs)\n",
        "\n",
        "model_config = PatchTSTConfigObject(**patchtst_config)\n",
        "\n",
        "# Instantiate the model\n",
        "# The model's __init__ takes 'configs' as the argument\n",
        "model = PatchTST(model_config).to(device) # <-- Pass the config object\n",
        "\n",
        "# Initialize Optimizer and Loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "print(f\"PatchTST model instantiated and moved to {device}.\")\n",
        "print(f\"Total parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
        "\n",
        "# --- 2. Define Helper Functions for Training and Evaluation ---\n",
        "\n",
        "def train_epoch(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for x_batch, y_batch in loader:\n",
        "        # Move data to the device\n",
        "        x_batch = x_batch.to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # --- Forward pass ---\n",
        "        y_pred = model(x_batch) # This will just return the forecast\n",
        "\n",
        "        # Calculate loss (only on the target variable 'OT')\n",
        "        target_col_index = list(df_train.columns).index(config.target_col)\n",
        "\n",
        "        loss = criterion(y_pred[..., target_col_index], y_batch[..., target_col_index])\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "def evaluate(model, loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for x_batch, y_batch in loader:\n",
        "            x_batch = x_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "\n",
        "            y_pred = model(x_batch) # Just the forecast\n",
        "\n",
        "            target_col_index = list(df_train.columns).index(config.target_col)\n",
        "            loss = criterion(y_pred[..., target_col_index], y_batch[..., target_col_index])\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "# --- 3. Main Training Loop ---\n",
        "\n",
        "print(\"--- Starting PatchTST Training ---\")\n",
        "n_epochs = config.n_epochs_patchtst  # <-- FIXED (removed extra underscore)\n",
        "best_val_loss = float('inf')\n",
        "model_save_path = os.path.join(PROJECT_DIR, \"best_patchtst.pth\")\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss = train_epoch(model, train_loader, optimizer, criterion)\n",
        "    val_loss = evaluate(model, val_loader, criterion)\n",
        "\n",
        "    end_time = time.time()\n",
        "    epoch_mins = (end_time - start_time) / 60\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins:.2f}m')\n",
        "    print(f'\\tTrain Loss: {train_loss:.6f}')\n",
        "    print(f'\\t Val. Loss: {val_loss:.6f}')\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), model_save_path)\n",
        "        print(f'\\t>> Validation loss decreased. Saving model to {model_save_path}')\n",
        "\n",
        "print(\"--- PatchTST Training Complete ---\")\n",
        "print(f\"Best validation loss: {best_val_loss:.6f}\")\n",
        "print(f\"Model saved to {model_save_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHmzLsgnZxCs",
        "outputId": "49d128bd-5f3d-48e9-e070-3c8014519959"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PatchTST model instantiated and moved to cuda.\n",
            "Total parameters: 807662\n",
            "--- Starting PatchTST Training ---\n",
            "Epoch: 01 | Time: 0.02m\n",
            "\tTrain Loss: 0.594873\n",
            "\t Val. Loss: 0.419190\n",
            "\t>> Validation loss decreased. Saving model to /content/drive/MyDrive/PatchCPT_Project/best_patchtst.pth\n",
            "Epoch: 02 | Time: 0.01m\n",
            "\tTrain Loss: 0.533031\n",
            "\t Val. Loss: 0.407908\n",
            "\t>> Validation loss decreased. Saving model to /content/drive/MyDrive/PatchCPT_Project/best_patchtst.pth\n",
            "Epoch: 03 | Time: 0.01m\n",
            "\tTrain Loss: 0.489636\n",
            "\t Val. Loss: 0.406947\n",
            "\t>> Validation loss decreased. Saving model to /content/drive/MyDrive/PatchCPT_Project/best_patchtst.pth\n",
            "Epoch: 04 | Time: 0.01m\n",
            "\tTrain Loss: 0.449407\n",
            "\t Val. Loss: 0.420905\n",
            "Epoch: 05 | Time: 0.01m\n",
            "\tTrain Loss: 0.414680\n",
            "\t Val. Loss: 0.459323\n",
            "Epoch: 06 | Time: 0.01m\n",
            "\tTrain Loss: 0.378597\n",
            "\t Val. Loss: 0.499953\n",
            "Epoch: 07 | Time: 0.01m\n",
            "\tTrain Loss: 0.344527\n",
            "\t Val. Loss: 0.519413\n",
            "Epoch: 08 | Time: 0.01m\n",
            "\tTrain Loss: 0.325528\n",
            "\t Val. Loss: 0.514217\n",
            "Epoch: 09 | Time: 0.01m\n",
            "\tTrain Loss: 0.308510\n",
            "\t Val. Loss: 0.510134\n",
            "Epoch: 10 | Time: 0.01m\n",
            "\tTrain Loss: 0.292361\n",
            "\t Val. Loss: 0.497908\n",
            "Epoch: 11 | Time: 0.01m\n",
            "\tTrain Loss: 0.272508\n",
            "\t Val. Loss: 0.512366\n",
            "Epoch: 12 | Time: 0.01m\n",
            "\tTrain Loss: 0.260776\n",
            "\t Val. Loss: 0.505017\n",
            "Epoch: 13 | Time: 0.01m\n",
            "\tTrain Loss: 0.244221\n",
            "\t Val. Loss: 0.499718\n",
            "Epoch: 14 | Time: 0.01m\n",
            "\tTrain Loss: 0.231004\n",
            "\t Val. Loss: 0.493824\n",
            "Epoch: 15 | Time: 0.01m\n",
            "\tTrain Loss: 0.226962\n",
            "\t Val. Loss: 0.492085\n",
            "Epoch: 16 | Time: 0.01m\n",
            "\tTrain Loss: 0.213584\n",
            "\t Val. Loss: 0.509846\n",
            "Epoch: 17 | Time: 0.01m\n",
            "\tTrain Loss: 0.206278\n",
            "\t Val. Loss: 0.501077\n",
            "Epoch: 18 | Time: 0.01m\n",
            "\tTrain Loss: 0.198221\n",
            "\t Val. Loss: 0.509318\n",
            "Epoch: 19 | Time: 0.01m\n",
            "\tTrain Loss: 0.191314\n",
            "\t Val. Loss: 0.536943\n",
            "Epoch: 20 | Time: 0.01m\n",
            "\tTrain Loss: 0.185673\n",
            "\t Val. Loss: 0.528167\n",
            "--- PatchTST Training Complete ---\n",
            "Best validation loss: 0.406947\n",
            "Model saved to /content/drive/MyDrive/PatchCPT_Project/best_patchtst.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 4"
      ],
      "metadata": {
        "id": "28DJK9chbyEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import sys\n",
        "\n",
        "\n",
        "sys.path.append('/content/PatchTST/PatchTST_supervised')\n",
        "\n",
        "# --- 2. Define the PatchCPT Backbone ---\n",
        "# We inherit from the 'PatchTST' class\n",
        "\n",
        "PatchTST_base_class = PatchTST\n",
        "\n",
        "class PatchCPT_Backbone(PatchTST_base_class):\n",
        "    def __init__(self, configs):\n",
        "        # We call the base class init directly.\n",
        "        PatchTST_base_class.__init__(self, configs)\n",
        "\n",
        "    def forward(self, x_enc):\n",
        "        # This is our override of the forward pass to capture 'z'.\n",
        "\n",
        "        # 1. RevIN\n",
        "        x = self.revin_layer(x_enc, 'norm')\n",
        "        x = x.permute(0, 2, 1)                                         # x: [B, C, L]\n",
        "\n",
        "        # 2. Patching\n",
        "        if self.padding_patch == 'end':\n",
        "            x = self.padding_patch_layer(x)\n",
        "        x = x.unfold(dimension=2, size=self.patch_len, step=self.stride) # x: [B, C, N, P]\n",
        "        x = x.permute(0, 1, 3, 2)                                      # x: [B, C, P, N]\n",
        "\n",
        "        # 3. Encoder -> This is z!\n",
        "        # --- THIS IS THE FIX ---\n",
        "        # The attribute is named 'backbone', not 'encoder'.\n",
        "        z, _ = self.backbone(x)                                    # z: [B, C, D, N]\n",
        "        x_for_head = z.permute(0, 1, 3, 2)                         # x: [B, C, N, D]\n",
        "\n",
        "        # 4. Head (Forecasting)\n",
        "        if self.individual:\n",
        "            y_pred = torch.zeros(x_for_head.shape[0], x_for_head.shape[1], self.head[0][3].out_features, device=x_for_head.device)\n",
        "            for i in range(x_for_head.shape[1]):\n",
        "                y_pred[:, i, :] = self.head[i](x_for_head[:, i, :, :])\n",
        "            y_pred = y_pred.permute(0, 2, 1) # [B, H, C]\n",
        "        else:\n",
        "            y_pred = self.head(x_for_head)   # [B, C, H]\n",
        "            y_pred = y_pred.permute(0, 2, 1) # [B, H, C]\n",
        "\n",
        "        # 5. Denorm\n",
        "        y_pred = self.revin_layer(y_pred, 'denorm')\n",
        "\n",
        "        # 6. Create pooled z_t (our \"regime\" vector)\n",
        "        # We pool over the PatchNum dimension (dim=-1)\n",
        "        # z shape is [B, C, D, N]\n",
        "        # z_t shape: [B, C, D] (Batch, Channels, d_model)\n",
        "        z_t = torch.mean(z, dim=-1)\n",
        "\n",
        "        return y_pred, z_t\n",
        "\n",
        "print(\"✅ PatchCPT_Backbone class defined.\")\n",
        "\n",
        "# --- 3. Instantiate Model and Load Weights ---\n",
        "\n",
        "# We need the same config object from Block 3\n",
        "model_config = PatchTSTConfigObject(**patchtst_config)\n",
        "\n",
        "# Instantiate our NEW model\n",
        "backbone = PatchCPT_Backbone(model_config).to(device)\n",
        "\n",
        "# Load the weights we just trained\n",
        "model_save_path = os.path.join(PROJECT_DIR, \"best_patchtst.pth\")\n",
        "backbone.load_state_dict(torch.load(model_save_path))\n",
        "backbone.eval() # Set to evaluation mode\n",
        "\n",
        "print(f\"Loaded trained weights into new backbone model from {model_save_path}\")\n",
        "\n",
        "# --- 4. Build the Calibration Memory ---\n",
        "\n",
        "print(\"Building calibration memory from 'val_loader'...\")\n",
        "\n",
        "all_z_t = []\n",
        "all_errors = []\n",
        "\n",
        "# Get the index of our target column\n",
        "target_col_index = list(df_train.columns).index(config.target_col)\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Use val_loader as the calibration set\n",
        "    for x_batch, y_batch in tqdm(val_loader, desc=\"Building Memory\"):\n",
        "        x_batch = x_batch.to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "\n",
        "        # Forward pass through our new backbone\n",
        "        y_pred, z_t = backbone(x_batch)\n",
        "\n",
        "        # --- Get Errors ---\n",
        "        # We only care about errors on the target column\n",
        "        # Shape: [B, PredLen]\n",
        "        errors = y_batch[..., target_col_index] - y_pred[..., target_col_index]\n",
        "\n",
        "        # --- Get Latent States ---\n",
        "        # We only need the latent state for the target channel\n",
        "        # Shape: [B, d_model]\n",
        "        z_t_target = z_t[:, target_col_index, :]\n",
        "\n",
        "        all_errors.append(errors.cpu())\n",
        "        all_z_t.append(z_t_target.cpu())\n",
        "\n",
        "# --- 5. Concatenate and Save Memory ---\n",
        "\n",
        "# Concatenate all batches\n",
        "# memory_z shape: [TotalValSamples, d_model]\n",
        "memory_z = torch.cat(all_z_t, dim=0)\n",
        "\n",
        "# memory_e shape: [TotalValSamples, PredLen]\n",
        "memory_e = torch.cat(all_errors, dim=0)\n",
        "\n",
        "# Save the memory tensors to Google Drive\n",
        "memory_z_path = os.path.join(PROJECT_DIR, \"memory_z.pt\")\n",
        "memory_e_path = os.path.join(PROJECT_DIR, \"memory_e.pt\")\n",
        "\n",
        "torch.save(memory_z, memory_z_path)\n",
        "torch.save(memory_e, memory_e_path)\n",
        "\n",
        "print(\"--- Memory Build Complete ---\")\n",
        "print(f\"Latent state memory (z) shape: {memory_z.shape}\")\n",
        "print(f\"Error memory (e) shape:       {memory_e.shape}\")\n",
        "print(f\"Memory saved to {PROJECT_DIR}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170,
          "referenced_widgets": [
            "9874016d81bd494786f09af1876b25ea",
            "b9bc4166eafd48258b7a24f1364490c4",
            "3b6f35fe2191443c9008a80ef2893de0",
            "68bd4037a4b64e5dbb9b5735605c8142",
            "ee0b99e7f7ae4380a8c75171611f42ff",
            "f54d28d88b9d49cb81443b59fb83ed6f",
            "b4e67f915dcc4b79a4de0cbb752c33e9",
            "396951c9d0794d8a9d953082f7ec055e",
            "8b5fd9c065e84d46927708cc2176ee9e",
            "319718c9c91e4def903bcbb61109ff51",
            "83a5c75314ab44e3b3b58ff8a63ab8c4"
          ]
        },
        "id": "py2i-QAvaM2d",
        "outputId": "5a92c7b8-6f91-4230-a904-1499bec9f5ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ PatchCPT_Backbone class defined.\n",
            "Loaded trained weights into new backbone model from /content/drive/MyDrive/PatchCPT_Project/best_patchtst.pth\n",
            "Building calibration memory from 'val_loader'...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Building Memory:   0%|          | 0/32 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9874016d81bd494786f09af1876b25ea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Memory Build Complete ---\n",
            "Latent state memory (z) shape: torch.Size([1001, 128])\n",
            "Error memory (e) shape:       torch.Size([1001, 96])\n",
            "Memory saved to /content/drive/MyDrive/PatchCPT_Project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Section 5"
      ],
      "metadata": {
        "id": "fjog53HIfsR3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# --- 1. Define the HopCPT Uncertainty Model ---\n",
        "# This module is just a wrapper for the Hopfield layer's\n",
        "# learnable W_Q and W_K matrices.\n",
        "\n",
        "class HopCPT_Module(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_heads):\n",
        "        super().__init__()\n",
        "        # We only need the Hopfield layer.\n",
        "        # The 'input_size' is our d_model (128).\n",
        "        # We set value_size = 1 because we want it to output a single\n",
        "        # weight for each error, but the Hopfield class we defined\n",
        "        # doesn't work that way. We'll use the attention weights directly.\n",
        "\n",
        "        # Let's instantiate the Hopfield layer from Block 1\n",
        "        self.hopfield = Hopfield(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_heads=num_heads,\n",
        "            dropout=0.1\n",
        "        )\n",
        "\n",
        "        # This is a hyperparameter from the paper.\n",
        "        # It controls the \"sharpness\" of the similarity search.\n",
        "        self.beta = nn.Parameter(torch.tensor(1.0))\n",
        "\n",
        "    def forward(self, z_query, z_memory):\n",
        "        # z_query shape: [B, D]\n",
        "        # z_memory shape: [M, D] (M = memory size)\n",
        "\n",
        "        # Hopfield layer expects batch dimension: [B, SeqLen, Dim]\n",
        "        # Query: [B, 1, D]\n",
        "        z_q = z_query.unsqueeze(1)\n",
        "\n",
        "        # Memory: [B, M, D]\n",
        "        # We need to expand the memory to match the batch size\n",
        "        z_k = z_memory.unsqueeze(0).expand(z_q.shape[0], -1, -1)\n",
        "\n",
        "        # The Hopfield layer's forward pass is (query, key, value)\n",
        "        # We only care about the Query-Key similarity, so we can\n",
        "        # just pass the keys as the values.\n",
        "        # The output 'attn_weights' is what we want.\n",
        "        # Shape: [B, NumHeads, 1, M]\n",
        "        _, attn_weights = self.hopfield(z_q, z_k, z_k, return_raw_associations=False)\n",
        "\n",
        "        # Average over the heads and squeeze\n",
        "        # Shape: [B, M]\n",
        "        attn_weights = attn_weights.mean(dim=1).squeeze(1)\n",
        "\n",
        "        return attn_weights\n",
        "\n",
        "# --- 2. Load Memory and Prepare for Training ---\n",
        "print(\"Loading calibration memory...\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "memory_z_path = os.path.join(PROJECT_DIR, \"memory_z.pt\")\n",
        "memory_e_path = os.path.join(PROJECT_DIR, \"memory_e.pt\")\n",
        "\n",
        "memory_z = torch.load(memory_z_path).to(device).float()\n",
        "memory_e = torch.load(memory_e_path).to(device).float()\n",
        "\n",
        "# We only train the HopCPT module on a *single* forecast step.\n",
        "# Let's use the first prediction step (t+1) as the target.\n",
        "memory_e_step = memory_e[:, 0] # Shape: [M]\n",
        "memory_e_step_abs = memory_e_step.abs() # Shape: [M]\n",
        "\n",
        "print(f\"Loaded memory_z: {memory_z.shape}\")\n",
        "print(f\"Loaded memory_e (for t+1): {memory_e_step.shape}\")\n",
        "\n",
        "# Create a simple dataloader for this\n",
        "# Each sample is (z_t, e_t)\n",
        "memory_dataset = torch.utils.data.TensorDataset(memory_z, memory_e_step_abs)\n",
        "memory_loader = torch.utils.data.DataLoader(memory_dataset, batch_size=config.batch_size, shuffle=True)\n",
        "\n",
        "# --- 3. Instantiate HopCPT Model and Optimizer ---\n",
        "hopcpt_model = HopCPT_Module(\n",
        "    input_size=config.d_model,       # 128\n",
        "    hidden_size=config.d_model,      # 128\n",
        "    num_heads=config.n_heads         # 8\n",
        ").to(device)\n",
        "\n",
        "optimizer = optim.Adam(hopcpt_model.parameters(), lr=config.learning_rate)\n",
        "\n",
        "# This is the auxiliary MSE loss from HopCPT (Eq. 8)\n",
        "# It trains the model to predict the current error |e_t|\n",
        "# by taking a weighted average of past errors |e_i|.\n",
        "def hopcpt_loss_fn(weights, past_errors, current_error):\n",
        "    # weights shape: [B, M]\n",
        "    # past_errors shape: [M]\n",
        "    # current_error shape: [B]\n",
        "\n",
        "    # Expand past_errors to batch: [B, M]\n",
        "    past_errors_batch = past_errors.unsqueeze(0).expand(weights.shape[0], -1)\n",
        "\n",
        "    # Calculate weighted average of past errors: [B]\n",
        "    pred_error = torch.sum(weights * past_errors_batch, dim=1)\n",
        "\n",
        "    # Return MSE loss\n",
        "    loss = nn.MSELoss()\n",
        "    return loss(pred_error, current_error)\n",
        "\n",
        "# --- 4. Main Training Loop for HopCPT ---\n",
        "print(\"--- Starting HopCPT Module Training ---\")\n",
        "n_epochs = config.n_epochs_hopcpt\n",
        "hopcpt_save_path = os.path.join(PROJECT_DIR, \"best_hopcpt.pth\")\n",
        "best_hopcpt_loss = float('inf')\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    start_time = time.time()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for z_batch, e_batch in memory_loader:\n",
        "        # z_batch shape: [B, D]\n",
        "        # e_batch shape: [B]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Get the similarity weights\n",
        "        # We are querying the *entire* memory_z\n",
        "        # Shape: [B, M] (M = total memory size)\n",
        "        weights = hopcpt_model(z_batch, memory_z)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = hopcpt_loss_fn(weights, memory_e_step_abs, e_batch)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(memory_loader)\n",
        "    epoch_time = time.time() - start_time\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_time:.2f}s | Loss: {avg_loss:.6f}')\n",
        "\n",
        "    if avg_loss < best_hopcpt_loss:\n",
        "        best_hopcpt_loss = avg_loss\n",
        "        torch.save(hopcpt_model.state_dict(), hopcpt_save_path)\n",
        "        print(f'\\t>> Loss decreased. Saving model to {hopcpt_save_path}')\n",
        "\n",
        "print(\"--- HopCPT Training Complete ---\")\n",
        "print(f\"Best HopCPT loss: {best_hopcpt_loss:.6f}\")\n",
        "print(f\"HopCPT module saved to {hopcpt_save_path}\")"
      ],
      "metadata": {
        "id": "21rcvi4Wb0Jn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49640aed-ba7d-4ad4-a31b-32a47e4cfc87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading calibration memory...\n",
            "Loaded memory_z: torch.Size([1001, 128])\n",
            "Loaded memory_e (for t+1): torch.Size([1001])\n",
            "--- Starting HopCPT Module Training ---\n",
            "Epoch: 01 | Time: 0.17s | Loss: 0.033073\n",
            "\t>> Loss decreased. Saving model to /content/drive/MyDrive/PatchCPT_Project/best_hopcpt.pth\n",
            "Epoch: 02 | Time: 0.10s | Loss: 0.032279\n",
            "\t>> Loss decreased. Saving model to /content/drive/MyDrive/PatchCPT_Project/best_hopcpt.pth\n",
            "Epoch: 03 | Time: 0.09s | Loss: 0.030795\n",
            "\t>> Loss decreased. Saving model to /content/drive/MyDrive/PatchCPT_Project/best_hopcpt.pth\n",
            "Epoch: 04 | Time: 0.08s | Loss: 0.030701\n",
            "\t>> Loss decreased. Saving model to /content/drive/MyDrive/PatchCPT_Project/best_hopcpt.pth\n",
            "Epoch: 05 | Time: 0.07s | Loss: 0.031066\n",
            "Epoch: 06 | Time: 0.08s | Loss: 0.030861\n",
            "Epoch: 07 | Time: 0.08s | Loss: 0.031074\n",
            "Epoch: 08 | Time: 0.07s | Loss: 0.030342\n",
            "\t>> Loss decreased. Saving model to /content/drive/MyDrive/PatchCPT_Project/best_hopcpt.pth\n",
            "Epoch: 09 | Time: 0.08s | Loss: 0.030535\n",
            "Epoch: 10 | Time: 0.08s | Loss: 0.031203\n",
            "Epoch: 11 | Time: 0.08s | Loss: 0.030366\n",
            "Epoch: 12 | Time: 0.08s | Loss: 0.030210\n",
            "\t>> Loss decreased. Saving model to /content/drive/MyDrive/PatchCPT_Project/best_hopcpt.pth\n",
            "Epoch: 13 | Time: 0.08s | Loss: 0.030415\n",
            "Epoch: 14 | Time: 0.08s | Loss: 0.030598\n",
            "Epoch: 15 | Time: 0.08s | Loss: 0.030199\n",
            "\t>> Loss decreased. Saving model to /content/drive/MyDrive/PatchCPT_Project/best_hopcpt.pth\n",
            "Epoch: 16 | Time: 0.08s | Loss: 0.030601\n",
            "Epoch: 17 | Time: 0.08s | Loss: 0.030325\n",
            "Epoch: 18 | Time: 0.08s | Loss: 0.030364\n",
            "Epoch: 19 | Time: 0.08s | Loss: 0.030725\n",
            "Epoch: 20 | Time: 0.08s | Loss: 0.030308\n",
            "--- HopCPT Training Complete ---\n",
            "Best HopCPT loss: 0.030199\n",
            "HopCPT module saved to /content/drive/MyDrive/PatchCPT_Project/best_hopcpt.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Section 6"
      ],
      "metadata": {
        "id": "JkAfYq84gb0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def weighted_quantile(values, weights, quantiles):\n",
        "    \"\"\"\n",
        "    Computes weighted quantiles. (Corrected Version)\n",
        "\n",
        "    :param values: torch.Tensor of shape [M], the values (e.g., past errors).\n",
        "    :param weights: torch.Tensor of shape [B, M], weights for each value.\n",
        "    :param quantiles: torch.Tensor of shape [Q], the quantiles to compute (e.g., [0.025, 0.975]).\n",
        "    :return: torch.Tensor of shape [B, Q], the computed quantile values.\n",
        "    \"\"\"\n",
        "    # values shape: [M]\n",
        "    # weights shape: [B, M]\n",
        "    # quantiles shape: [Q]\n",
        "\n",
        "    # Ensure weights are non-negative and sum to 1\n",
        "    weights = torch.clamp(weights, min=0)\n",
        "    weights_sum = torch.sum(weights, dim=1, keepdim=True)\n",
        "    weights = weights / (weights_sum + 1e-9) # Normalize\n",
        "\n",
        "    # Sort values\n",
        "    sorted_values, sort_indices = torch.sort(values, dim=0)\n",
        "\n",
        "    # Reorder weights based on sorted values\n",
        "    # [B, M]\n",
        "    sorted_weights = torch.gather(weights, dim=1, index=sort_indices.unsqueeze(0).expand_as(weights))\n",
        "\n",
        "    # Compute cumulative weights\n",
        "    # [B, M]\n",
        "    cum_weights = torch.cumsum(sorted_weights, dim=1)\n",
        "\n",
        "\n",
        "    # We need to find the indices for each batch element.\n",
        "    # We make 'quantiles' have the same batch dimension as 'cum_weights'.\n",
        "    # cum_weights shape: [B, M]\n",
        "    # quantiles_batch shape: [B, Q]\n",
        "    quantiles_batch = quantiles.unsqueeze(0).expand(cum_weights.shape[0], -1)\n",
        "\n",
        "    # Find the indices where quantiles would be inserted\n",
        "    # This call is now valid: boundaries=[B, M], values=[B, Q]\n",
        "    quantile_indices = torch.searchsorted(cum_weights, quantiles_batch)\n",
        "\n",
        "    # Clamp indices to be valid (0 to M-1)\n",
        "    # Shape is [B, Q], so no .squeeze() is needed.\n",
        "    quantile_indices = torch.clamp(quantile_indices, 0, values.shape[0] - 1)\n",
        "\n",
        "    # Gather the sorted values at these indices\n",
        "    final_quantiles = torch.gather(sorted_values.unsqueeze(0).expand(weights.shape[0], -1),\n",
        "                                   dim=1,\n",
        "                                   index=quantile_indices)\n",
        "\n",
        "    return final_quantiles\n",
        "\n",
        "print(\"✅ weighted_quantile function defined (Corrected).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H27-Tk-Qft6p",
        "outputId": "60db52b2-cf27-4f97-c7cb-82c3be4ce772"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ weighted_quantile function defined (Corrected).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchCPT(nn.Module):\n",
        "    def __init__(self, backbone_config, hopcpt_config, backbone_weights_path, hopcpt_weights_path, memory_z_path, memory_e_path, target_col_index, alpha, device):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.target_col_index = target_col_index\n",
        "        self.alpha = alpha\n",
        "\n",
        "        # 1. Load Forecasting Backbone\n",
        "        self.backbone = PatchCPT_Backbone(backbone_config).to(device)\n",
        "        self.backbone.load_state_dict(torch.load(backbone_weights_path))\n",
        "        self.backbone.eval()\n",
        "        print(\"Loaded trained PatchTST backbone.\")\n",
        "\n",
        "        # 2. Load HopCPT Module\n",
        "        self.hopcpt_model = HopCPT_Module(**hopcpt_config).to(device)\n",
        "        self.hopcpt_model.load_state_dict(torch.load(hopcpt_weights_path))\n",
        "        self.hopcpt_model.eval()\n",
        "        print(\"Loaded trained HopCPT module.\")\n",
        "\n",
        "        # 3. Load Memory (Keys and Values)\n",
        "        self.memory_z = torch.load(memory_z_path).to(device).float() # [M, D]\n",
        "        self.memory_e = torch.load(memory_e_path).to(device).float() # [M, PredLen]\n",
        "        print(f\"Loaded memory_z ({self.memory_z.shape}) and memory_e ({self.memory_e.shape})\")\n",
        "\n",
        "        # Define quantiles\n",
        "        self.quantiles = torch.tensor([self.alpha / 2, 1 - (self.alpha / 2)], device=self.device) # e.g., [0.025, 0.975]\n",
        "\n",
        "    def forward(self, x_batch):\n",
        "        # x_batch shape: [B, SeqLen, C]\n",
        "\n",
        "        # --- Step 1 & 2: Get Forecast and Latent State ---\n",
        "        with torch.no_grad():\n",
        "            y_pred, z_t = self.backbone(x_batch)\n",
        "\n",
        "            # Isolate the target variable's forecast and latent state\n",
        "            y_pred_target = y_pred[..., self.target_col_index] # [B, PredLen]\n",
        "            z_t_target = z_t[:, self.target_col_index, :]       # [B, D]\n",
        "\n",
        "        # --- Step 3: Query Hopfield for Weights ---\n",
        "        with torch.no_grad():\n",
        "            # Get similarity weights\n",
        "            # Shape: [B, M] (M = memory size)\n",
        "            weights = self.hopcpt_model(z_t_target, self.memory_z)\n",
        "\n",
        "        # --- Step 4: Get Weighted Quantiles ---\n",
        "        # We must do this for each step in the prediction horizon (pred_len)\n",
        "\n",
        "        all_quantiles = []\n",
        "        for i in range(config.pred_len):\n",
        "            # Get errors for this specific step: [M]\n",
        "            step_errors = self.memory_e[:, i]\n",
        "\n",
        "            # Compute quantiles for this step: [B, 2]\n",
        "            # (2 = low and high quantiles)\n",
        "            step_quantiles = weighted_quantile(step_errors, weights, self.quantiles)\n",
        "            all_quantiles.append(step_quantiles)\n",
        "\n",
        "        # Stack into [B, PredLen, 2]\n",
        "        quantiles = torch.stack(all_quantiles, dim=1)\n",
        "\n",
        "        # q_low: [B, PredLen], q_high: [B, PredLen]\n",
        "        q_low = quantiles[..., 0]\n",
        "        q_high = quantiles[..., 1]\n",
        "\n",
        "        # --- Step 5: Final Output ---\n",
        "        # The intervals are Y_pred + q\n",
        "        lower_bound = y_pred_target + q_low\n",
        "        upper_bound = y_pred_target + q_high\n",
        "\n",
        "        return y_pred_target, lower_bound, upper_bound\n",
        "\n",
        "print(\"✅ PatchCPT class defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZITboDvgeBj",
        "outputId": "94a21b97-1f8d-4491-d530-551373f5a5c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ PatchCPT class defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Starting Inference Smoke Test ---\")\n",
        "\n",
        "# --- 1. Define Configs ---\n",
        "# We need the configs again to instantiate the models\n",
        "\n",
        "# PatchTST config\n",
        "patchtst_config_obj = PatchTSTConfigObject(**patchtst_config)\n",
        "\n",
        "# HopCPT config\n",
        "hopcpt_config_dict = {\n",
        "    'input_size': config.d_model,\n",
        "    'hidden_size': config.d_model,\n",
        "    'num_heads': config.n_heads\n",
        "}\n",
        "\n",
        "# --- 2. Define Paths ---\n",
        "backbone_path = os.path.join(PROJECT_DIR, \"best_patchtst.pth\")\n",
        "hopcpt_path = os.path.join(PROJECT_DIR, \"best_hopcpt.pth\")\n",
        "mem_z_path = os.path.join(PROJECT_DIR, \"memory_z.pt\")\n",
        "mem_e_path = os.path.join(PROJECT_DIR, \"memory_e.pt\")\n",
        "\n",
        "# Target column index\n",
        "target_idx = list(df_train.columns).index(config.target_col)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "try:\n",
        "    # --- 3. Instantiate Final Model ---\n",
        "    patch_cpt_model = PatchCPT(\n",
        "        backbone_config=patchtst_config_obj,\n",
        "        hopcpt_config=hopcpt_config_dict,\n",
        "        backbone_weights_path=backbone_path,\n",
        "        hopcpt_weights_path=hopcpt_path,\n",
        "        memory_z_path=mem_z_path,\n",
        "        memory_e_path=mem_e_path,\n",
        "        target_col_index=target_idx,\n",
        "        alpha=config.alpha,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    # --- 4. Get One Test Batch ---\n",
        "    x_test, y_test = next(iter(test_loader))\n",
        "    x_test = x_test.to(device)\n",
        "    y_test = y_test.to(device)\n",
        "\n",
        "    # --- 5. Run Inference ---\n",
        "    y_pred, y_lower, y_upper = patch_cpt_model(x_test)\n",
        "\n",
        "    print(\"\\n--- ✅ Inference Smoke Test SUCCESS ---\")\n",
        "    print(f\"Input x shape:  {x_test.shape}\")\n",
        "    print(f\"Output y_pred shape:  {y_pred.shape}\")\n",
        "    print(f\"Output y_lower shape: {y_lower.shape}\")\n",
        "    print(f\"Output y_upper shape: {y_upper.shape}\")\n",
        "\n",
        "    # Check one prediction\n",
        "    print(\"\\nExample Prediction (Batch 0, Step 0):\")\n",
        "    print(f\"  Forecast: {y_pred[0, 0].item():.4f}\")\n",
        "    print(f\"  True:     {y_test[0, 0, target_idx].item():.4f}\")\n",
        "    print(f\"  Interval: [{y_lower[0, 0].item():.4f}, {y_upper[0, 0].item():.4f}]\")\n",
        "\n",
        "    is_covered = (y_test[0, 0, target_idx] >= y_lower[0, 0]) & (y_test[0, 0, target_idx] <= y_upper[0, 0])\n",
        "    print(f\"  Covered:  {is_covered.item()}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n--- ❌ Inference Smoke Test FAILED ---\")\n",
        "    print(f\"Error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nM46J-IFgf4K",
        "outputId": "43986e5e-9f0b-47bc-8c95-9ec0e0635730"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Inference Smoke Test ---\n",
            "Loaded trained PatchTST backbone.\n",
            "Loaded trained HopCPT module.\n",
            "Loaded memory_z (torch.Size([1001, 128])) and memory_e (torch.Size([1001, 96]))\n",
            "\n",
            "--- ✅ Inference Smoke Test SUCCESS ---\n",
            "Input x shape:  torch.Size([32, 96, 7])\n",
            "Output y_pred shape:  torch.Size([32, 96])\n",
            "Output y_lower shape: torch.Size([32, 96])\n",
            "Output y_upper shape: torch.Size([32, 96])\n",
            "\n",
            "Example Prediction (Batch 0, Step 0):\n",
            "  Forecast: -6.0237\n",
            "  True:     -5.9890\n",
            "  Interval: [-6.5528, -5.5090]\n",
            "  Covered:  True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1615431587.py:41: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at /pytorch/aten/src/ATen/native/BucketizationUtils.h:32.)\n",
            "  quantile_indices = torch.searchsorted(cum_weights, quantiles_batch)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Section 7"
      ],
      "metadata": {
        "id": "BnJlcb8Eg8cP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# --- 1. Define Metric Functions ---\n",
        "\n",
        "def calculate_metrics(y_true, y_pred, y_lower, y_upper, alpha):\n",
        "    \"\"\"\n",
        "    Calculates all key metrics for the project.\n",
        "    \"\"\"\n",
        "\n",
        "    # --- 1. Accuracy Metrics (MSE, MAE) ---\n",
        "    mse = torch.mean((y_true - y_pred)**2).item()\n",
        "    mae = torch.mean(torch.abs(y_true - y_pred)).item()\n",
        "\n",
        "    # --- 2. Coverage (PIC) ---\n",
        "    is_covered = (y_true >= y_lower) & (y_true <= y_upper)\n",
        "    pic = torch.mean(is_covered.float()).item()\n",
        "\n",
        "    # --- 3. Coverage Gap (Delta Cov) ---\n",
        "    # How far is the coverage from the target (e.g., 95%)\n",
        "    target_coverage = 1.0 - alpha\n",
        "    coverage_gap = pic - target_coverage\n",
        "\n",
        "    # --- 4. Prediction Interval Width (PI-Width) ---\n",
        "    # Measures the sharpness/efficiency of the intervals\n",
        "    pi_width = torch.mean(y_upper - y_lower).item()\n",
        "\n",
        "    # --- 5. Winkler Score ---\n",
        "    # The main SOTA metric that combines coverage and sharpness.\n",
        "    # Penalizes for being too wide, and severely penalizes for missing.\n",
        "\n",
        "    penalty_miss_low = (y_lower - y_true) * (y_true < y_lower)\n",
        "    penalty_miss_high = (y_true - y_upper) * (y_true > y_upper)\n",
        "\n",
        "    # Winkler Score = (width) + (penalty for low) + (penalty for high)\n",
        "    # The (2 / alpha) term is the severe penalty for missing.\n",
        "    winkler_score = pi_width + (2 / alpha) * (penalty_miss_low + penalty_miss_high)\n",
        "    winkler_score = torch.mean(winkler_score).item()\n",
        "\n",
        "    metrics = {\n",
        "        'MSE': mse,\n",
        "        'MAE': mae,\n",
        "        'PIC': pic,\n",
        "        'Delta_Cov': coverage_gap,\n",
        "        'PI_Width': pi_width,\n",
        "        'Winkler_Score': winkler_score\n",
        "    }\n",
        "    return metrics\n",
        "\n",
        "print(\"✅ Metric functions defined.\")\n",
        "\n",
        "# --- 2. Run Full Evaluation Loop ---\n",
        "\n",
        "print(\"--- Starting Full Test Set Evaluation ---\")\n",
        "\n",
        "# Instantiate the final model (if not already done)\n",
        "# You can skip this if 'patch_cpt_model' is still in memory from Block 6.3\n",
        "try:\n",
        "    patch_cpt_model\n",
        "    print(\"Model already in memory.\")\n",
        "except NameError:\n",
        "    print(\"Instantiating model...\")\n",
        "    patchtst_config_obj = PatchTSTConfigObject(**patchtst_config)\n",
        "    hopcpt_config_dict = {\n",
        "        'input_size': config.d_model,\n",
        "        'hidden_size': config.d_model,\n",
        "        'num_heads': config.n_heads\n",
        "    }\n",
        "    target_idx = list(df_train.columns).index(config.target_col)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    patch_cpt_model = PatchCPT(\n",
        "        backbone_config=patchtst_config_obj,\n",
        "        hopcpt_config=hopcpt_config_dict,\n",
        "        backbone_weights_path=os.path.join(PROJECT_DIR, \"best_patchtst.pth\"),\n",
        "        hopcpt_weights_path=os.path.join(PROJECT_DIR, \"best_hopcpt.pth\"),\n",
        "        memory_z_path=os.path.join(PROJECT_DIR, \"memory_z.pt\"),\n",
        "        memory_e_path=os.path.join(PROJECT_DIR, \"memory_e.pt\"),\n",
        "        target_col_index=target_idx,\n",
        "        alpha=config.alpha,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "# --- Collect all predictions ---\n",
        "all_y_true = []\n",
        "all_y_pred = []\n",
        "all_y_lower = []\n",
        "all_y_upper = []\n",
        "\n",
        "patch_cpt_model.eval()\n",
        "with torch.no_grad():\n",
        "    for x_batch, y_batch in tqdm(test_loader, desc=\"Evaluating on Test Set\"):\n",
        "        x_batch = x_batch.to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "\n",
        "        # Run inference\n",
        "        y_pred, y_lower, y_upper = patch_cpt_model(x_batch)\n",
        "\n",
        "        # Get the true 'y' for the target column\n",
        "        y_true_target = y_batch[..., target_idx]\n",
        "\n",
        "        # Store results\n",
        "        all_y_true.append(y_true_target.cpu())\n",
        "        all_y_pred.append(y_pred.cpu())\n",
        "        all_y_lower.append(y_lower.cpu())\n",
        "        all_y_upper.append(y_upper.cpu())\n",
        "\n",
        "# Concatenate all batches into single tensors\n",
        "y_true = torch.cat(all_y_true, dim=0)\n",
        "y_pred = torch.cat(all_y_pred, dim=0)\n",
        "y_lower = torch.cat(all_y_lower, dim=0)\n",
        "y_upper = torch.cat(all_y_upper, dim=0)\n",
        "\n",
        "print(\"Evaluation complete.\")\n",
        "print(f\"Total samples evaluated: {y_true.shape[0]}\")\n",
        "print(f\"Prediction shape: {y_pred.shape}\")\n",
        "\n",
        "# --- 3. Calculate and Print Final Metrics ---\n",
        "\n",
        "# We can calculate metrics for the whole horizon, or for a specific step.\n",
        "# Let's start with the average across all 96 steps.\n",
        "final_metrics = calculate_metrics(y_true, y_pred, y_lower, y_upper, config.alpha)\n",
        "\n",
        "print(\"\\n--- 📊 Final Metrics (Avg. across all 96 steps) ---\")\n",
        "print(f\"  🎯 Accuracy (MSE):    {final_metrics['MSE']:.4f}\")\n",
        "print(f\"  🎯 Accuracy (MAE):    {final_metrics['MAE']:.4f}\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"  ✅ Coverage (PIC):    {final_metrics['PIC'] * 100:.2f}%  (Target: {(1-config.alpha)*100}%)\")\n",
        "print(f\"  📊 Coverage Gap:      {final_metrics['Delta_Cov'] * 100:+.2f}%\")\n",
        "print(f\"  📏 Interval Width:    {final_metrics['PI_Width']:.4f}\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"  🏆 Winkler Score:     {final_metrics['Winkler_Score']:.4f}  (Lower is better)\")\n",
        "print(\"-------------------------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344,
          "referenced_widgets": [
            "5944ff39937c48d0979f91ce8d62ab22",
            "12f1d6fa7cd54731a53077b782fe8492",
            "cfd4049d29bd4323902e7a348e3d351a",
            "56a062c53ef444d7bc84ee02ab25c0e3",
            "2c4cd550418745a4ab22e7cfa14a12b4",
            "4ab8f41bf38c474bb3514e0d082eec9b",
            "cb09e80fbb254030af578ef03f2a3461",
            "e21bb38764c444db87aeb8b31d8f744d",
            "ce557f3828e84a56881ecf322cb58571",
            "a9c33546a173472c8d9d081d65b4f6da",
            "68959dff99d84856bd15862e4229cbb9"
          ]
        },
        "id": "owoayMZcghiC",
        "outputId": "30c87c68-9ce3-4a90-eca6-fdc1878c2533"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Metric functions defined.\n",
            "--- Starting Full Test Set Evaluation ---\n",
            "Model already in memory.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating on Test Set:   0%|          | 0/32 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5944ff39937c48d0979f91ce8d62ab22"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation complete.\n",
            "Total samples evaluated: 1001\n",
            "Prediction shape: torch.Size([1001, 96])\n",
            "\n",
            "--- 📊 Final Metrics (Avg. across all 96 steps) ---\n",
            "  🎯 Accuracy (MSE):    0.2983\n",
            "  🎯 Accuracy (MAE):    0.4248\n",
            "----------------------------------------\n",
            "  ✅ Coverage (PIC):    96.86%  (Target: 95.0%)\n",
            "  📊 Coverage Gap:      +1.86%\n",
            "  📏 Interval Width:    2.4039\n",
            "----------------------------------------\n",
            "  🏆 Winkler Score:     2.6341  (Lower is better)\n",
            "-------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FQM4s2THg-a7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}